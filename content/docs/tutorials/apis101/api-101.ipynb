{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOdLmOPJFZlL"
      },
      "source": [
        "# APIs 101 (oDCM)\n",
        "\n",
        "*The focus in this tutorial lies on pagination (i.e., looping through multiple pages), and parameters (i.e., modifying the response of an API call). We know you love dad jokes, so guess what? We're back with many more jokes, and you're going to learn how to save them all! Finally, we show you how to obtain user-level data from the Reddit!*\n",
        "\n",
        "--- \n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "* Send HTTP requests to a web API, and retrieve JSON responses\n",
        "* Use parameters to modify the results of an API call\n",
        "* Iterate over multiple pages of JSON responses \n",
        "* Extract and store results of an API request in lists and files\n",
        "\n",
        "--- \n",
        "\n",
        "## Acknowledgements\n",
        "This course draws on a variety of online resources which can be retrieved from the [course website](https://odcm.hannesdatta.com/#student-profile--prerequisites). \n",
        "\n",
        "\n",
        "--- \n",
        "\n",
        "## Support Needed?\n",
        "For technical issues outside of scheduled classes, please check the [support section](https://odcm.hannesdatta.com/docs/course/support) on the course website.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvtTUHpyFZlP"
      },
      "source": [
        "## 1. Icanhazdajoke\n",
        "\n",
        "### 1.1 Make an API request\n",
        "\n",
        "[Icanhazdadjoke.com](https://icanhazdadjoke.com) is a simple web site that allows users of their API to receive (randomized) *dad jokes*. Yes, we know that sounds stupid, but we like that API for its simplicity, which is ideal when explaining to you more about APIs.\n",
        "\n",
        "So, the code cell below calls the joke API, and the result of the API request displays a joke. \n",
        "\n",
        "__Let's try it out__\n",
        "\n",
        "Run the cell a few times to notice that with each call, you see a new joke.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sLdCCJCFZlR"
      },
      "outputs": [],
      "source": [
        "# request JSON output from icanhazdadjoke API\n",
        "import requests\n",
        "url = \"https://icanhazdadjoke.com\"\n",
        "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
        "joke_request = response.json() \n",
        "print(joke_request)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndaVRgUGFZlS"
      },
      "source": [
        "### 1.2 Use parameters to modify the API results   \n",
        "\n",
        "__Importance__\n",
        "\n",
        "Probably you agree that dad jokes per se aren't that exciting. Wouldn't it be amazing to search for particular jokes instead?\n",
        "\n",
        "APIs certainly provide the functionality to *customize* requests. That's where APIs make most of a difference! You have probably already modified the results of an API call a dozen times without even knowing it. For example, if you Google the word `cat`, the results page may look something like this:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/apis101/images/google.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvHItyB4FZlU"
      },
      "source": [
        "Note how the URL in the browser starts with [`google.com/search?q=cat...`](https://www.google.com/search?q=cat)? What happened here is that your search query was passed to the Google Search API, and hence returned the results of the search query `cat`. That search query is even already embedded in the link itself. Cool, right?\n",
        "\n",
        "__Let's try it out__\n",
        "\n",
        "So, rather than filling out the search box on the website of Icanhazdadjoke.com itself, you can also tweak it in the URL directly. Open your browser now at [https://icanhazdadjoke.com/search?term=cat](https://icanhazdadjoke.com/search?term=cat), and modify the `term` parameter to try a search for different jokes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MokLrriFZlV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/apis101/images/cat_jokes.gif\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5lb2pCBFZlW"
      },
      "source": [
        "With the idea of passing parameters to a website, we can update the `search_url` and include the `params` attribute, which contains a dictionary with parameters that further specify our request. Run the cell below to see cat jokes here in Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_DLOkCPFZlX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "search_url = \"https://icanhazdadjoke.com/search\"\n",
        "\n",
        "response = requests.get(search_url, \n",
        "                        headers={\"Accept\": \"application/json\"}, \n",
        "                        params={\"term\": \"cat\"})\n",
        "joke_request = response.json()\n",
        "print(joke_request)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca0f1qTLFZlY"
      },
      "source": [
        "The `joke_request` object now contains a list with all cat-related jokes (`joke_request['results']`), the search term (`cat`), and the total number of jokes (`10`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AutE_HyeFZla"
      },
      "source": [
        "#### Exercise 1\n",
        "1. Change the search term parameter to `dog` and revisit `joke_request['results']`. How many dog jokes are there? \n",
        "2. Write a function `find_joke()` that takes a query as an input parameter and returns the number of jokes from the `icanhazdadjoke` search API (tip: use your answer to question 1 as a starting point!). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwC-BOsGFZlc"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvAlLOoYFZld"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEXwoq01FZld"
      },
      "outputs": [],
      "source": [
        "# Question 1 \n",
        "search_url = \"https://icanhazdadjoke.com/search\"\n",
        "\n",
        "response = requests.get(search_url, \n",
        "                        headers={\"Accept\": \"application/json\"}, \n",
        "                        params={\"term\": \"dog\"})\n",
        "joke_request = response.json()\n",
        "print(f\"The number of dog jokes is: {joke_request['total_jokes']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1DWeEwRFZlf"
      },
      "outputs": [],
      "source": [
        "# Question 2\n",
        "def find_jokes(term):\n",
        "    search_url = \"https://icanhazdadjoke.com/search\"\n",
        "\n",
        "    response = requests.get(search_url, \n",
        "                            headers={\"Accept\": \"application/json\"}, \n",
        "                            params={\"term\": term})\n",
        "    joke_request = response.json()\n",
        "    num_results = joke_request['total_jokes']\n",
        "    return num_results\n",
        "\n",
        "find_jokes(\"some-searchterm-you-would-like-to-try-out\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNEjDv5JFZlf"
      },
      "source": [
        "### 1.3 Pagination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj6ZvGzBFZlg"
      },
      "source": [
        "__Importance__\n",
        "\n",
        "Transferring data is costly - not strictly in a monetary sense, but in *time*. So - APIs are typically very greedy in returning data. Ideally, they only produce a very targeted data point that is needed for the user to see. On icanhazdadjoke.com, for example, that would be a few jokes at maximum. It saves the web site owner paying for bandwidth and guarantees that the site responds fast to user input (such as navigating the site or searching for jokes).\n",
        "\n",
        "However, when using APIs for research purposes, we are frequently interested in obtaining *everything*. What's the use, for example, to get a book's most recent ten reviews, if there are hundreds of reviews written?\n",
        "\n",
        "We think you see where we're going with this... \n",
        "\n",
        "__Let's try it out__\n",
        "\n",
        "So, let's try to grab all of the 649 jokes currently available at Icanhazdadjoke.com. The API output, unfortunately, only shows the *first 20 jokes*. To retrieve the remaining 629 jokes, you need *pagination*. The API divides the data into smaller subsets that can be accessed on various pages, rather than returning all output at once. \n",
        "\n",
        "Let's retrieve the first batch of dad jokes (note, here we're searching for the `term` `\"\"` - an empty string - which brings us to the entire set of jokes available via the API. In practice, searching for `\"\"` is often blocked by APIs - simply because the site doesn't *want* you to extract a complete copy of their data. In that case, you'd have to become creative to obtain your seeds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKv6t1KNFZlg"
      },
      "outputs": [],
      "source": [
        "search_url = \"https://icanhazdadjoke.com/search\"\n",
        "\n",
        "response = requests.get(search_url, \n",
        "                        headers={\"Accept\": \"application/json\"}, \n",
        "                        params={\"term\": \"\"})\n",
        "joke_request = response.json()\n",
        "joke_request['results'] = '' # let's remove all jokes, and only look at the other attributes in the JSON response\n",
        "joke_request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXmVj2iAFZlh"
      },
      "source": [
        "You notice that by default, each page contains 20 jokes (see `limit` in the JSON response above), where page 1 shows jokes 1 to 20, page 2 jokes 21 to 40, ..., and page 33 jokes 641 to 649. \n",
        "\n",
        "You can adjust the number of results on each page (max. 30) with the `limit` parameter (e.g., `params={\"limit\": 10}`). In practice, almost every API on the web limits the results of an API call (`100` is also a common cap).\n",
        "\n",
        "In the example below, we set `limit` equal to `10`, `20`, and `30`, and see how it affects the number of total pages (`total_pages`) on which jokes are listed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUw23rokFZlh"
      },
      "outputs": [],
      "source": [
        "for limit in range(10, 31, 10):  # note that range(a, b) runs from a to b-1; so the last value is exclusive (so from 10 to 30 with steps of 10)\n",
        "    response = requests.get(search_url, \n",
        "                            headers={\"Accept\": \"application/json\"}, \n",
        "                            params={\"term\": \"\", \n",
        "                                   \"limit\": limit})\n",
        "    joke_request = response.json()\n",
        "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEnOCU6FZlh"
      },
      "source": [
        "As expected, we find that the higher the limit, the more results fit on a single page, and thus the *lower the number of pages* to loop through."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMK9tfGOFZli"
      },
      "source": [
        "--- \n",
        "#### Exercise 2\n",
        "\n",
        "In addition to the `limit` parameter, you can specify the current page number with the `page` parameter (e.g., `params={\"term\": \"\", \"page\": 2}`. See the example in the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6e-NI5TFZli"
      },
      "outputs": [],
      "source": [
        "response = requests.get(search_url, \n",
        "                            headers={\"Accept\": \"application/json\"}, \n",
        "                            params={\"term\": \"\", \n",
        "                                   \"limit\": 5,\n",
        "                                   \"page\": 2})\n",
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9gYQS2xFZli"
      },
      "source": [
        "Adapt the function `find_joke()` (see question 2 of exercise 1), such that it loops over *all available pages*, and stores the ids and jokes in a list. You can leave the `limit` parameter at its default value (20). Make sure that your function also works when you pass it a search `term`. \n",
        "\n",
        "Tip: To determine how many pages you need to loop through, you can use the `total_pages` field (e.g., there are only ten cat jokes, so in that case, 1 page would suffice)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9nCZOk1FZlj"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP9fJlULFZlk"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdokwdZYFZlk"
      },
      "outputs": [],
      "source": [
        "def find_jokes(term):\n",
        "    search_url = \"https://icanhazdadjoke.com/search\"\n",
        "    page = 1\n",
        "    jokes = []\n",
        "\n",
        "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
        "        response = requests.get(search_url, \n",
        "                                headers={\"Accept\": \"application/json\"}, \n",
        "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
        "                                        \"page\": page})\n",
        "        joke_request = response.json()\n",
        "        jokes.extend(joke_request['results'])\n",
        "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
        "            page += 1\n",
        "        else: \n",
        "            return jokes\n",
        "\n",
        "output = find_jokes(\"cat\") # try running it with \"\", too!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO7riom2FZll"
      },
      "outputs": [],
      "source": [
        "print(f\"You've collected {len(output)} jokes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj85K3h9FZll"
      },
      "source": [
        "### 1.4 Wrap-up\n",
        "\n",
        "To sum up, we have seen how *parameters* can be a powerful tool when working with APIs. They allow you to tailor your request to be more specific or loop through multiple pages. \n",
        "\n",
        "In the API documentation, you typically find more information about the available parameters and the values they can take on. For example, the `icanhazdadjoke` [documentation](https://icanhazdadjoke.com/api) includes a section on the `/search` endpoint and the accepted parameters (`page`, `limit`, `term`). These parameters, however, differ from one API to another. So it's crucial to study each web service's API documentation carefully.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnHbmBSJFZll"
      },
      "source": [
        "--- \n",
        "## 2. Reddit\n",
        "\n",
        "### 2.1 Subreddits\n",
        "\n",
        "[Reddit](https://reddit.com) is a widespread American social news aggregation and discussion site. The service uses an API to generate the website's content and grants public access to the API.\n",
        "\n",
        "In this tutorial, we zoom in on \"subreddits\", which are niche communities centered around a particular topic. Users can nearly post anything in these subreddits, and you'd be surprised to find out what people are talking about. For example, see below for a screenshot of the [subreddit on Science](https://www.reddit.com/r/Science).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/apis101/images/reddit_science.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-tIvpmeFZlm"
      },
      "source": [
        "__Let's try it out__\n",
        "\n",
        "Subreddits all start with `reddit.com/r/...`. Here are a few examples:\n",
        "\n",
        "- [askreddit](https://www.reddit.com/r/AskReddit), \n",
        "- [aww](https://www.reddit.com/r/aww/), \n",
        "- [gifs](https://www.reddit.com/r/gifs/), \n",
        "- [showerthoughts](https://www.reddit.com/r/Showerthoughts), \n",
        "- [lifehacks](https://www.reddit.com/r/lifehacks), \n",
        "- [getmotivated](https://www.reddit.com/r/GetMotivated), \n",
        "- [moviedetails](https://www.reddit.com/r/MovieDetails), \n",
        "- [todayilearned](https://www.reddit.com/r/todayilearned/), \n",
        "- [foodporn](https://www.reddit.com/r/FoodPorn/). \n",
        "\n",
        "Take your time to browse through some of the subreddits, and get familiar with the structure of the pages.\n",
        "\n",
        "After a while, you'd probably notice that subreddits are hosted by moderators, who monitor whether the posts adhere to a set of (informal) rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iinD1ZRaFZlm"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/apis101/images/reddit_moderators.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzP5miLbFZlm"
      },
      "source": [
        "For example, links to papers you share in [`r/science`](https://www.reddit.com/r/science/) must be less than 6 months old. \n",
        "\n",
        "Other users can join a subreddit so that they receive updates about new posts and comments.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SlPYk0nFZln"
      },
      "source": [
        "#### Exercise 3\n",
        "Consult the [`marketing`](https://www.reddit.com/r/marketing/hot/) subreddit and answer the following questions: \n",
        "1. For your thesis, you need to collect survey responses. Are you allowed to share a link to your survey in this subreddit? Please elaborate on how you came to this conclusion. \n",
        "2. You post a link (and wonder how many users will potentially be able to see your post). How many users are subscribed to the subreddit? How many users are currently online?\n",
        "3. Like other social media platforms, you can navigate towards Reddit's user-profiles and learn more about these persons. Inspect the profile of one of the users who has posted on the Reddit (actually, it is one of the moderators), [`sixwaystop313`](https://www.reddit.com/user/sixwaystop313). Describe in your own words what types of information you can gather from this user. How is the feed organized?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MnNnUOkFZln"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZE0JxHyFZlo"
      },
      "source": [
        "#### Solutions\n",
        "1. No, the subreddit rules prescribe users not to post surveys and homework assignments (right sidebar).\n",
        "2. `r/marketing` is moderated has about 370k members, and (at the time of writing this tutorial), about 160 of them were online.\n",
        "3. On a user page, you find the bio, trophies, communities the user moderates, connected accounts, and most importantly: all user's posts and comments.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CPsnVXQFZlo"
      },
      "source": [
        "### 2.2 API headers  \n",
        "\n",
        "**Importance**  \n",
        "\n",
        "Let's now obtain some of the data we have seen on the \"About\" page of the `marketing` thread, using the  Reddit API. \n",
        "\n",
        "To request data from the Reddit API, we need to include `headers` in our request. HTTP headers are a vital part of any API request, containing *meta-data associated with the request* (e.g., type of browser, language, expected data format, etc.). \n",
        "\n",
        "**Let's try it out**  \n",
        "\n",
        "Below we request the about page of the [`marketing`]() subreddit that includes such a header. We make our first request to the Reddit API and parse the output in the upcoming exercise!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSgj6wtcFZlp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = 'https://www.reddit.com/r/marketing/about/.json'\n",
        "\n",
        "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
        "response = requests.get(url, headers=headers)\n",
        "json_response = response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2hc3y3QFZlp"
      },
      "source": [
        "#### Exercise 4\n",
        "1. First, take a look at the `json_response` object. Then, leave out the `headers` parameter in your request (so it becomes `requests.get(url)` instead), rerun the cell, and inspect the `json_response` another time. Are there any differences? \n",
        "2. Write a while-loop that prints the count of the number of currently active users of the `marketing` subreddit. Have your code pause every 5 seconds before refreshing. Stop the loop after 3 iterations. For pausing, use the function `time.sleep(5)`. Import the time package using `import time`.\n",
        "3. Convert your code from the previous exercise into a function `get_usercount()` that takes a `subreddit` as input and returns the total number of users, and the number of currently active users as a dictionary. Test your function for the `science`, `skateboarding`, and `marketing` subreddits. How many total and currently active users do these communities have?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbYMSlJ8FZlq"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKCdv0a-FZlq"
      },
      "source": [
        "#### Solutions\n",
        "1. Without the `headers` parameter, the API returns an error code (429). Headers are frequently used to track who is using the API. The user of the \"anonymous header\" has pushed the boundaries too much!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbyFLh1aFZlr"
      },
      "outputs": [],
      "source": [
        "# Question 2 \n",
        "import time\n",
        "\n",
        "i = 1\n",
        "while i <= 3:\n",
        "    url = 'https://www.reddit.com/r/marketing/about/.json'\n",
        "    headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    json_response = response.json()\n",
        "    \n",
        "    print(json_response['data']['active_user_count'])\n",
        "    i += 1\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6LguOiYFZls"
      },
      "outputs": [],
      "source": [
        "# Question 3\n",
        "def get_usercount(subreddit):\n",
        "    response = response = requests.get(f'https://www.reddit.com/r/{subreddit}/about/.json', headers=headers)\n",
        "    json_response = response.json()\n",
        "    out = {}\n",
        "    out['subreddit'] = subreddit\n",
        "    out['total_users'] = json_response['data']['subscribers']\n",
        "    out['active_users'] = json_response['data']['active_user_count']\n",
        "    return out\n",
        "    \n",
        "get_usercount('science')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxV5mKuGFZlt"
      },
      "outputs": [],
      "source": [
        "get_usercount('skateboarding')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQHlDxCOFZlu"
      },
      "outputs": [],
      "source": [
        "get_usercount('marketing')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yut4Ft_FZlu"
      },
      "source": [
        "---\n",
        "### 2.3 Profile pages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olq3puLWFZlv"
      },
      "source": [
        "In addition to subreddits (`r/...`) and about pages (`.../about/`), Reddit users have their own profile page. Let's have another look at the marketing moderator [profile](https://www.reddit.com/user/sixwaystop313) we saw before. Each of the `children` in the `data` is characterized by a type (e.g., `t1` = comment, `t3` = post; for details see [API documentation](https://redditclient.readthedocs.io/en/latest/reference/)), subreddit, timestamp, number of comments, upvotes, downvotes, and many others. \n",
        "\n",
        "__Let's try it out__\n",
        "\n",
        "Run the API call below, and browse the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA6Ri9jrFZlv"
      },
      "outputs": [],
      "source": [
        "mod = \"sixwaystop313\"\n",
        "response = requests.get(f'https://www.reddit.com/user/{mod}.json', headers=headers)\n",
        "json_response = response.json()\n",
        "json_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbspUXjLFZlv"
      },
      "source": [
        "That's a whole lot of output, which is difficult to go through. So let's copy it to a [JSON viewer](https://jsonviewer.stack.hu). Before we can do that, we have to replace the Pythonic `None`, `True` and `False` by strings (JSON viewer throws an error otherwise)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KOaCICQFZlw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "json_response = json.loads(response.text.replace('null', '\"None\"').replace('True','\"True\"').replace('False','\"False\"'))\n",
        "json_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmamlPR2FZlw"
      },
      "source": [
        "#### Exercise 5\n",
        "1. The `json_response` object contains both comments and posts ordered chronologically (exactly as they appear on the profile page). Pick a comment (`kind`: `'t1'`) of the author and store the text of the comment in a variable called `comment_text`. \n",
        "2. What happens to `comments_text` once the author publishes another post? \n",
        "3. How many objects are stored in `json_response['data']['children']`? What does that mean? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUv09ygTFZlx"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im2_nd6eFZlx"
      },
      "source": [
        "#### Solutions\n",
        "1. At the moment of creating this solutions file, the 1st item in the list is a comment which we extract as follows:\n",
        "`comment_text = json_response['data']['children'][0]['data']`. In your case, it may be 2nd (or 3rd, 4th, ... item), however, provided that all other items in the lists are posts. For that reason, the counter after `[0]` may deviate from time to time. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-9l8SF6FZly"
      },
      "outputs": [],
      "source": [
        "# if this solution throws a \"KeyError: body\" error it means the most recent JSON object is not a comment of kind t1 (so change the 0 for 1, 2, ... until it runs) - see question 2\n",
        "comment_text = json_response['data']['children'][0]['data']['body']\n",
        "comment_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BgFwcTuFZly"
      },
      "source": [
        "2. Since the list items are ordered chronologically, new items are appended at the *beginning* of the list and thus push existing items to the \"right\" (i.e., index 0 becomes index 1, etc.). Suppose that the author publishes another post, then index `[0]` would no longer contain a comment. Post items have been structured differently from comment items, which could potentially break your script once you try to parse non-existing items. For example, posts do not have a `['body']` element that stores the comment text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQwwQvv7FZly"
      },
      "source": [
        "3. The object comprises 25 items, which implies that only the 25 most recent comments and posts are shown. Thus, we need to apply pagination to obtain historical records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgVHZXPzFZlz"
      },
      "outputs": [],
      "source": [
        "len(json_response['data']['children'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKdMO7XUFZlz"
      },
      "source": [
        "### 2.4 Pagination\n",
        "\n",
        "__Importance__\n",
        "\n",
        "As you've just noticed, the API only returns a subset of all records (every time you scroll to the bottom of the page, it pulls in new data - ordered chronologically). After all, it would take ages to show all data for a user that has been active on Reddit since 2009! \n",
        "\n",
        "__Let's try it out__\n",
        "\n",
        "Similar to `icanhazdadjoke`, we apply pagination to tell the API which part of the data to return. The difference, however, is that it's not a number (like `\"page\": 2`) but a string of characters that can only be obtained from the previous request (i.e., we cannot derive what the next key will be from a pattern, like page 2, 3, ..., etc.). The request we already made contains this \"secret\" key in the attribute `after`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwvcfMAIFZl0"
      },
      "outputs": [],
      "source": [
        "json_response['data']['after']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArL2Dt8-FZl0"
      },
      "source": [
        "Next, we attach this key to our request with the `after` parameter to obtain the next subset of items and assign the responses to a variable called `json_response_after`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9uqvYfqFZl0"
      },
      "outputs": [],
      "source": [
        "after = json_response['data']['after']\n",
        "url = f'https://www.reddit.com/user/{mod}.json'\n",
        "response = requests.get(url, \n",
        "                        headers=headers, \n",
        "                        params={\"after\": after})\n",
        "json_response_after = response.json()\n",
        "json_response_after"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6-Ytf9kFZl1"
      },
      "source": [
        "At the point of writing this tutorial (when you're doing this tutorial it's likely different!), the last item in `json_respose` is the following post (`Detroit's Brewing Heritage' on tap at Historical Museum`): "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chPfznmUFZl1"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/apis101/images/json_response.png\" width=60% align=\"left\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36kvrzwHFZl1"
      },
      "source": [
        "The first and second items in `json_response_after` are the two comments below that (\"Shame on ... us back.\" and \"Are you ... comment /u/ehchip\"). In other words, where one object ends, another begins. We apply this concept to loop over the first ten pages. At each iteration, we store the `after` attribute, which we use as a parameter in the follow-up request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1CbcP1VFZl2"
      },
      "outputs": [],
      "source": [
        "after = None\n",
        "item_type = []\n",
        "\n",
        "for counter in range(10): \n",
        "    url = f'https://www.reddit.com/user/{mod}.json'\n",
        "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
        "    response = requests.get(url, \n",
        "                            headers=headers, \n",
        "                            params={\"after\": after})\n",
        "    json_response = response.json()\n",
        "    after = json_response['data']['after'] \n",
        "\n",
        "    # loop over all items in a request\n",
        "    for item in json_response['data']['children']:\n",
        "        item_type.append(item['kind'])\n",
        "\n",
        "# Let's view the item types: \n",
        "item_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teVsdjNKFZl2"
      },
      "source": [
        "#### Exercise 6\n",
        "1. Why do we define `after = None` at the top of the file? Can we leave it out? \n",
        "2. Without looking at the list's length, how many items do you expect in `item_type`? \n",
        "3. Of all items in `item_type`, calculate the percentage of posts (`t3`) and comments (`t1`). What does this tell you? \n",
        "4. Convert the code snippet above into a function `reddit_activity()` that takes a `username`, `attribute`, and `num_pages` as inputs and returns the attribute for the given user. For example, `reddit_activity(\"sixwaystop313\", \"subreddit_name_prefixed\", 40)` should return a list of the subreddits in which the user has posted or commented across the 1000 most recent items. \n",
        "5. Use the function written in 3 to assess whether the moderator has actively contributed to the `r/marketing` subreddit recently?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UPY-bbHFZl2"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRvL7RSgFZl2"
      },
      "source": [
        "#### Solutions \n",
        "1. In our first request we don't know the value of `after` yet. It is important, however, to include this line because otherwise the `after` value in `params={}` is undefined. \n",
        "2. We expect the list to have a size of 10 (number of requests) * 25 (number of items per request) = 250. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylvPOd0NFZl3"
      },
      "outputs": [],
      "source": [
        "# Question 3\n",
        "def item_frequency(items, item_filter):\n",
        "    total_items = len(items)\n",
        "    item_filter_count = items.count(item_filter)\n",
        "    return item_filter_count / total_items * 100\n",
        "            \n",
        "perc_posts = item_frequency(item_type, 't1')\n",
        "perc_comments = item_frequency(item_type, 't3')\n",
        "\n",
        "print(f\"The percentage of posts and comments is {perc_posts}% and {perc_comments}%, respectively\")\n",
        "# Thus, based on this subset of data, the author is more likely to start a new post than to comment on others' posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxAwnbmdFZl3"
      },
      "outputs": [],
      "source": [
        "# Question 4\n",
        "def reddit_activity(username, attribute, num_pages):\n",
        "    after = None\n",
        "    activity = []\n",
        "\n",
        "    for counter in range(num_pages): \n",
        "        url = f'https://www.reddit.com/user/{username}.json'\n",
        "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
        "        response = requests.get(url, \n",
        "                                headers=headers, \n",
        "                                params={\"after\": after})\n",
        "        json_response = response.json()\n",
        "        after = json_response['data']['after']\n",
        "\n",
        "        # loop over all items in a request\n",
        "        for item in json_response['data']['children']:\n",
        "            activity.append(item['data'][attribute])\n",
        "    return activity\n",
        "\n",
        "reddit_data = reddit_activity(\"sixwaystop313\", \"subreddit_name_prefixed\", 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuINQa6sFZl4"
      },
      "outputs": [],
      "source": [
        "reddit_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjmVwPAaFZl4"
      },
      "outputs": [],
      "source": [
        "print(f\"The percentage of posts and comments in the marketing subreddit is {item_frequency(reddit_data, 'r/marketing')}%\")\n",
        "# Thus, the moderator has not actively contributed to the marketing subreddit recently"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKBbOH38FZl4"
      },
      "source": [
        "---\n",
        "### 2.5 Time Conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZgmBl4CFZl5"
      },
      "source": [
        "__Importance__\n",
        "\n",
        "When retrieving data from an API - in particular *user-level data*, not only the content of a post or comment matters, but also when the comment or post was written. In other words, we seek to extract the date and time (timestamps) from a users' comments and posts on Reddit.\n",
        "\n",
        "__Let's try it out__\n",
        "\n",
        "Run the cell below, to extract the timestamp from `sixwaystop313`'s most recent activity on Reddit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov6IvQJ1FZl_"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.reddit.com/user/sixwaystop313.json'\n",
        "response = requests.get(url, headers=headers)\n",
        "response.json()['data']['children'][0]['data']['created_utc']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvYUC-xoFZl_"
      },
      "source": [
        "Hm... is *that* really a timestamp?\n",
        "\n",
        "Well... computers handle time differently than humans. And what programmers somewhat converged to is that timestamps are best measured in *the number of seconds passed since 1 January, 1970*. With the use of the `time` library, we can easily convert it into a readable date and time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC1xGKz9FZmA"
      },
      "outputs": [],
      "source": [
        "import time \n",
        "time_example = response.json()['data']['children'][0]['data']['created_utc']\n",
        "time_converted = time.gmtime(time_example)\n",
        "print(time_converted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNn-39piFZmA"
      },
      "source": [
        "From `time_converted` you can extract the day, month, and year separately:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bAzUNefTFZmB"
      },
      "outputs": [],
      "source": [
        "print(f\"The day is: {time_converted.tm_mday}\")\n",
        "print(f\"The month is: {time_converted.tm_mon}\")\n",
        "print(f\"The year is: {time_converted.tm_year}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMiwXjSHFZmB"
      },
      "source": [
        "Or together, like this (characters that start with `%` have a special meaning, the `-` in  between these characters are literally the dashes you see in the output): "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nERIhHf3FZmB"
      },
      "outputs": [],
      "source": [
        "print(time.strftime(\"%d-%m-%Y\", time_converted))  \n",
        "# %d = day\n",
        "# %m = month\n",
        "# %Y = year (4 digits) and %y = year (2 digits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54nx-H-SFZmC"
      },
      "source": [
        "--- \n",
        "#### Exercise 7 \n",
        "1. In a similar way, you can convert the UTC time into an hour (`%H`) and minute (`%M`). Transform `time_example` into a readable time. The output should be `06:17`. \n",
        "2. Suppose we want to analyze the Reddit use of `sixwaystop313` throughout the day. More specifically, we want to know during what hours the user is most active on the platform. \n",
        "  * Use the function `reddit_activity()` you wrote earlier to pull in the UTC timestamps (set `num_items` to `10`). \n",
        "  * Extract the hour from these timestamps. \n",
        "  * Determine the top 3 hours the user is most active on Reddit. You can assume that the total number of posts and comments is a reasonable proxy for time spend on the platform. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdR25Fg4FZmC"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fef_eBnSFZmC"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HvYNd-PFZmD"
      },
      "outputs": [],
      "source": [
        "# Question 1 \n",
        "print(time.strftime(\"%H:%M\", time_converted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjRmtDjqFZmD"
      },
      "outputs": [],
      "source": [
        "# Question 2\n",
        "time_data = reddit_activity(\"sixwaystop313\", \"created_utc\", 10)\n",
        "hours = []\n",
        "\n",
        "for timestamp in time_data: \n",
        "    time_converted = time.gmtime(timestamp)\n",
        "    hours.append(time_converted.tm_hour)\n",
        "    \n",
        "for hour in range(24):\n",
        "    print(f\"Hour {hour}: {hours.count(hour)} items\")\n",
        "    \n",
        "# Check out which hours are listed most to find the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeXlkT3HFZmE"
      },
      "source": [
        "---\n",
        "\n",
        "### 2.6 Building an API extraction module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw_-nWJSFZmE"
      },
      "source": [
        "__Importance__\n",
        "\n",
        "Up to now, we've written functions that in itself carry out separate tasks: one for obtaining data from the subreddit's about page, and one to obtain particular attributes of users of the subreddit.\n",
        "\n",
        "However, when we use APIs for research, we are not so much interested in the results of \"single-shot\" API requests, but we would like to obtain a *copy* of the entire data, so that we can analyze it later.\n",
        "\n",
        "So, the purpose of this section is to \"stitch\" together individual API requests. For now, we assume that we are interested in studying how the posting behavior of users currently active on the channel influences the total number of active users of the community.\n",
        "\n",
        "In other words, we need to \n",
        "- obtain a list of all users who have currently posted on the subreddit (the first 25), and\n",
        "- store all of their posts and comments in a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkurf5JaFZmE"
      },
      "source": [
        "__Let's try it out__\n",
        "\n",
        "Let's first make use of a function `get_users()` that returns the currently active users from the `marketing` subreddit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxVezGaXFZmF"
      },
      "outputs": [],
      "source": [
        "def get_users(subreddit):\n",
        "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
        "    response = requests.get(url, \n",
        "                            headers=headers)\n",
        "    json_response = response.json()\n",
        "    users = []\n",
        "    # loop over all items in a request\n",
        "    for item in json_response['data']['children']:\n",
        "        users.append(item['data']['author'])\n",
        "    return users\n",
        "\n",
        "users = get_users('marketing')\n",
        "users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw3XaRB8FZmF"
      },
      "source": [
        "We also have written our `reddit_activity` function, which we could call now using the (for prototypical purposes) first moderator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDCvsz1zFZmF"
      },
      "outputs": [],
      "source": [
        "reddit_activity(users[0], \"created_utc\", 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIDLESWlFZmG"
      },
      "source": [
        "The code above produced a list of timestamps of when that user was active. The function right now is not particularly useful yet, because it doesn't return multiple fields (but only one). So, let's first extend the reddit_activity function to...\n",
        "- take on a `list` of attributes to extract, rather than just one\n",
        "- to store the results in a dictionary, rather than a list (easier for writing to CSV later), and\n",
        "- to always store the type of content (`kind`, e.g., post or comment).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOrCoqS1FZmG"
      },
      "outputs": [],
      "source": [
        "def reddit_activity_updated(username, attributes, num_pages):\n",
        "    after = None\n",
        "    activity = []\n",
        "\n",
        "    for counter in range(num_pages): \n",
        "        url = f'https://www.reddit.com/user/{username}.json'\n",
        "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
        "        response = requests.get(url, \n",
        "                                headers=headers, \n",
        "                                params={\"after\": after})\n",
        "        json_response = response.json()\n",
        "        after = json_response['data']['after']\n",
        "\n",
        "        # loop over all items in a request\n",
        "        for item in json_response['data']['children']:\n",
        "            tmp = {}\n",
        "            tmp['kind'] = item['kind']\n",
        "            for attribute in attributes:\n",
        "                try:\n",
        "                    tmp[attribute] = item['data'][attribute]\n",
        "                except:\n",
        "                    0 # do nothing\n",
        "            activity.append(tmp)\n",
        "    return activity\n",
        "\n",
        "reddit_data = reddit_activity_updated(users[0], [\"created_utc\", \"subreddit_name_prefixed\", \"body\"], 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQSLhM19FZmH"
      },
      "outputs": [],
      "source": [
        "reddit_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "954D9lNwFZmH"
      },
      "source": [
        "Observe now that the updated `reddit_activity_updated()` function returns multiple fields. The function also checks whether a field is actually *part* of the data (e.g., there is never a `body` for type `t3` (post), but only for `t1` (comments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S59gg9g1FZmH"
      },
      "source": [
        "__Exercise 8__\n",
        "\n",
        "1. Extend the `reddit_activity_updated()` function, so that it also stores a users' user name in the results.\n",
        "\n",
        "2. Write a loop that calls `reddit_activity_updated()` on all 25 most recent users of the subreddit `marketing`, and stores the result in a (long) list of dictionaries. Limit yourself to the first 10 pages per user (to save time doing this exercise)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhtT4ePIFZmH"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJJFYi4ZFZmI"
      },
      "source": [
        "__Solutions__\n",
        "\n",
        "*Question 1*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_UbUYq-FZmJ"
      },
      "outputs": [],
      "source": [
        "def reddit_activity_updated(username, attributes, num_pages):\n",
        "    after = None\n",
        "    activity = []\n",
        "\n",
        "    for counter in range(num_pages): \n",
        "        url = f'https://www.reddit.com/user/{username}.json'\n",
        "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
        "        response = requests.get(url, \n",
        "                                headers=headers, \n",
        "                                params={\"after\": after})\n",
        "        json_response = response.json()\n",
        "        after = json_response['data']['after']\n",
        "\n",
        "        # loop over all items in a request\n",
        "        for item in json_response['data']['children']:\n",
        "            tmp = {}\n",
        "            tmp['username'] = username # <-- SOLUTION TO QUESTION 1 IS HERE\n",
        "            tmp['kind'] = item['kind']\n",
        "            for attribute in attributes:\n",
        "                try:\n",
        "                    tmp[attribute] = item['data'][attribute]\n",
        "                except:\n",
        "                    0 # do nothing\n",
        "            activity.append(tmp)\n",
        "    return activity\n",
        "\n",
        "reddit_data = reddit_activity_updated(users[0], [\"created_utc\", \"subreddit_name_prefixed\", \"body\"], 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSEKybYmFZmK"
      },
      "outputs": [],
      "source": [
        "# let's preview the first few results\n",
        "reddit_data[0:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTOVlJlOFZmL"
      },
      "source": [
        "*Question 2*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhGVa4k2FZmM"
      },
      "outputs": [],
      "source": [
        "users = get_users('marketing')\n",
        "\n",
        "all_data = []\n",
        "for user in users:\n",
        "    print('processing content for moderator ' + user)\n",
        "    reddit_data = reddit_activity_updated(user, [\"created_utc\", \"subreddit_name_prefixed\", \"body\"], 10)\n",
        "    for item in reddit_data:\n",
        "        all_data.append(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_5f_N0yFZmM"
      },
      "outputs": [],
      "source": [
        "# preview the results\n",
        "print(len(all_data))\n",
        "all_data[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn35COPlFZmN"
      },
      "source": [
        "---\n",
        "\n",
        "### 2.7 Exporting data to CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv0aiethFZmN"
      },
      "source": [
        "__Importance__\n",
        "\n",
        "Alright, you've almost made it. We've accomplished a whole lot by now: we just wrote a tool that extracts user-level data, and appends all that data in a list of dictionaries stored in Python. But, how can we port the data to another software program (e.g., R, Excel)?\n",
        "\n",
        "We need to convert the data to a Comma Separated Values (CSV) file.\n",
        "\n",
        "More specifically, we'd like to have a file with five columns, containing:\n",
        "- the username,\n",
        "- the type of content (post vs. comment),\n",
        "- the timestamp (readable for humans),\n",
        "- the sub reddit name, and\n",
        "- the body, if present.\n",
        "\n",
        "To faciliate writing to a CSV file, we'll make use of the `csv` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_BFum9KFZmN"
      },
      "source": [
        "__Let's try it out__\n",
        "\n",
        "Here, we'll start with a code snippet to parse the `username`, content type (`kind`) and timestamp (`created_utc`) to a CSV file. Run the snippet and the next cells to see the result (it should create a new file `reddit_posts.csv` in your current working directory)! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPwweNcyFZmO"
      },
      "outputs": [],
      "source": [
        "import csv \n",
        "\n",
        "with open(\"reddit_posts.csv\", \"w\") as csv_file: # <<- this is the line with the \"flag\" see exercises below\n",
        "    writer = csv.writer(csv_file, delimiter = \";\")\n",
        "    writer.writerow([\"username\", \"kind\", \"created_utc\"])\n",
        "    for content in all_data:\n",
        "        writer.writerow([content['username'], content['kind'], content['created_utc']])\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vL2NSV9FZmO"
      },
      "source": [
        "Let's preview the content of that file directly in Jupyter, using the `pandas` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klUxfCE0FZmO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"reddit_posts.csv\", sep=\";\")\n",
        "# shows top 10 rows\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMlQugVyFZmP"
      },
      "source": [
        "Good job, so far. But the file is not ready yet, so let's work on the exercises by extending the code snippet above.\n",
        "\n",
        "---\n",
        "\n",
        "__Exercise 9__  \n",
        "The `reddit_posts.csv` file now only includes 3 columns (`userame`, `kind`, `created_utc`). Please add the following columns as well: the subreddit name, the text of users' comments, and the timestamp converted to YYYY-MM-DD (date, e.g., 2021-01-15) and HH:MM (e.g., 08:00). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj6I3EZ3FZmP"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xnIeeSlFZmP"
      },
      "source": [
        "__Solutions__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5ORolnfFZmQ"
      },
      "outputs": [],
      "source": [
        "import csv \n",
        "import time\n",
        "\n",
        "with open(\"reddit_posts.csv\", \"w\", encoding = \"utf-8\") as csv_file: # <<- this is the line with the \"flag\"l see exercises below\n",
        "    writer = csv.writer(csv_file, delimiter = \";\")\n",
        "    writer.writerow([\"username\", \"kind\", \"created_utc\", \"subreddit_name_prefixed\", \"date\", \"time\", \"body\"])\n",
        "    for content in all_data:\n",
        "        time_converted = time.gmtime(content['created_utc'])\n",
        "        datestamp = time.strftime(\"%Y-%m-%d\", time_converted)\n",
        "        timestamp = time.strftime(\"%H:%M\", time_converted)\n",
        "        try:\n",
        "            bodytext = content['body']\n",
        "        except:\n",
        "            bodytext = ''\n",
        "        writer.writerow([content['username'], content['kind'], content['created_utc'], content['subreddit_name_prefixed'],\n",
        "                        datestamp, timestamp, bodytext])\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "A-oukJXBFZmQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"reddit_posts.csv\", sep=\";\")\n",
        "# shows top 10 rows\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZvXSWsKFZmQ"
      },
      "source": [
        "### 2.8 Wrap-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQYGdPXkFZmQ"
      },
      "source": [
        "Good job - you've made it!\n",
        "\n",
        "After working on this set of exercises, you should be able to further explore the Reddit API on your own. Does `sixwaystop313` spend the most time in subreddits in which he gets the most upvotes? Did his posting behavior change over time? Are users that have posted recently more likely to be a premium Reddit user? Think what data is required to obtain such data, and then try to extract such data.\n",
        "\n",
        "At the same time, realize that we have only scratched the surface of what's possible with APIs. Headers and pagination played a vital role in requests and were sufficient thus far. Yet, the majority of [API endpoints](https://www.reddit.com/dev/api/) require authentication (OAuth), which is a whole topic on its own."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "api-101.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}